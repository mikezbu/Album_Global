import tensorflow as tf
from tensorflow import keras
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
import importlib

sentences = [
    'Importing into AWS',
    'Albums highest in popularity'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(word_index)

input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre'))

max_sequence_length = max([len(x) for x in input_sequences])
